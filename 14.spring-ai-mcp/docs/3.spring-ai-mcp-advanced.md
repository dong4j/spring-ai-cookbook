# 如何封装 MCP & 多厂商 LLM 差异

<!-- 
写作思路：
- 这一篇主角是：Spring AI MCP Client / Server 的封装原理。
- 重点讲两件事：
	1.	Spring AI MCP Client 如何从 MCP Server 拉工具，然后转成各个 LLM 的函数调用格式
	2.	LLM 发出的私有 function calling JSON，如何被 Spring AI 解析再转换回 MCP 的 tools.call
- 可以用简化版 Java 代码 + 时序图解释这两个方向的“schema 转换”。
- 最后给一些「高级用法」：多 MCP Server、多模型、同步/异步、错误处理。

大纲示例：
1.	回顾：我们现在知道 MCP 协议是怎样的
  - 再画一张总图：
	    - LLM ←→ Spring AI (ChatClient + MCP Client) ←→ MCP Server
  - 点名这一篇的重点：“中间那块的所有细节”
2.	Spring AI MCP Client 启动时做了什么？
  - Boot Starter 自动配置：创建 MCP Client Bean
  - 读取 application.yml 中的 MCP server 配置（stdio/http/sse）
  - 建立连接，发 initialize，拿到 tools 列表
  - 把 MCP tools 注册为 Spring AI 工具（tool callbacks）
3.	第一次 schema 转换：MCP tools → LLM function schema
  - MCP Server 返回的工具格式：name + description + inputSchema
    - Spring AI 如何把它转换为：
    - OpenAI 的 function 定义
    - 其他模型的 tool schema（描述概念级）
  - 用伪代码展示一个转换过程（例如 McpToolDefinition → OpenAiFunctionDefinition）
4.	第二次 schema 转换：LLM function calling → MCP tools.call
  - 当模型输出类似：
	    - function_call: { name: "search", arguments: "{...}" } 时
  - Spring AI 的 ChatClient 如何拦截这个工具调用
  - 如何构造成 MCP 的 tools.call JSON-RPC 请求
  - 发送给 MCP Client → 再转给 MCP Server
5.	高阶用法 & 细节
  - 多 MCP Server：配置多个连接 / 逻辑路由
  - 多 LLM：OpenAI + Anthropic + 本地模型同时存在时的调用路径
  - SYNC / ASYNC 客户端的差异与注意事项
  - 日志与调试建议
6.	小结：Spring AI 帮我们做了哪些「本该很麻烦」的事
  - 屏蔽不同厂商 function calling 差异
  - 屏蔽 MCP transport / JSON-RPC 细节
  - 把「协议 + 模型」层折叠成统一的编程体验
  - 下一篇开始，我们视角从 Spring AI / 规范，扩展到更大的「MCP 生态」。

过渡到第 5 篇：

MCP 不只是 Spring AI 的玩具，它正在变成一个多语言、多平台、多工具共享的生态。
下一篇，我们就来看看其他 MCP 相关产品：
- Nacos MCP 网关怎么把传统 REST API 变成 MCP Server
- codex 的 MCP Server 又是如何让任意 MCP Client 接入的？
-->

